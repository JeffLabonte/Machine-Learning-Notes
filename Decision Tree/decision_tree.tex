\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}
  \begin{titlepage}
    \begin{center}
      \line(1,0){300} \\
      [0.25in]
      \huge{\bfseries Decision Tree Notes}\\
      [2mm]
      \line(1,0){300}\\
      [1.5cm]
      \textsc{\LARGE Jeff labonte}\\
      [0.75cm]
    \end{center}
  \end{titlepage}

  \section{Information Entropy}
    \ \\[2mm]
    On this page I will note what I have learned throughout all the research I made
    towards better understanding of decision trees\\
    \\
    \textit{\textbf{Information Entropy}} : It is a concept from information theory. It tells how much Information
    there is in an event. In general, the more uncertain or random the event is, the more Information
    it will contain. More clearly stated, information is a decrease of incertainty or entropy \\
    \\
    [2mm]
    \textit{from \url{https://simple.wikipedia.org/wiki/Information_entropy}}
    \\
    [2mm]
    \\
    Basicly, it is the probality of the information that will comes out of a system,
    which will produce more and who is going to produce less informations.
    \\
    \textit{i.e letters generated by a machine that produces a sequence of letters.}
    \\
    [4mm]
    \\
    [5mm]
    \textbf{ Example and formulas took from \url{https://www.khanacademy.org/computing/computer-science/informationtheory/moderninfotheory/v/information-entropy}}\\
    \\
    [5mm]
    \\
    Example from Khah Academy : machine 1 has \\
    \textbullet{ \textbf{50\%} chance to produce the letter \textbf{A}}\\
    \textbullet{ \textbf{12.5\%} chance to produce the letter \textbf{B}}\\
    \textbullet{ \textbf{12.5\%} chance to produce the letter \textbf{C}}\\
    \textbullet{ \textbf{25\%} chance to produce the letter \textbf{D}}\\
    \\
    [4mm]
    Machine 2 has \\
    \textbullet{ \textbf{25\%} chance to produce the letter \textbf{A}}\\
    \textbullet{ \textbf{25\%} chance to produce the letter \textbf{B}}\\
    \textbullet{ \textbf{25\%} chance to produce the letter \textbf{C}}\\
    \textbullet{ \textbf{25\%} chance to produce the letter \textbf{D}}\\
    \\
    [4mm]
    The multiplier to the letter is found by finding the level at which the letter
    is in the tree of propablity.
    \\
    [2mm]
    \\
    For machine one :\\
    [1mm]
    \begin{math}
      \#bounces = p_a \times 1 + p_b \times 3 + p_c \times 3 + p_d \times 2\\
      \#bounces = 0.50 \times 1 + 0.125 \times 3 + 0.125 \times 3 + 0.25 \times 2\\
      \#bounces = 1.75\ bounces\ to\ reach\ an\ answer
    \end{math}
    \\
    [2mm]
    \\
    For machine two :\\
    [1mm]
    \begin{math}
      \#bounces = p_a \times 1 + p_b \times 3 + p_c \times 3 + p_d \times 2\\
      \#bounces = 0.25 \times 2 + 0.25 \times 2 + 0.25 \times 2 + 0.25 \times 2\\
      \#bounces = 2\ bounces\ to\ reach\ an\ answer
    \end{math}
    \\
    [4mm]
    \\
    So if we have to guess 200 values from the machine.\\
    [2mm]
    \textbullet{ We would have to ask 175 times machine 1}\\
    [1mm]
    \textbullet{ We would have to ask 200 times machine 2}\\
    [2mm]
    Which means machine 1 will produce less information. It also means
    that there less \textbf{incertainty} with \textit{machine 1}. This is called \textbf{entropy}
    \\
    [4mm]
    \\
    \textit{Entropy is the value H}\\
    \textit{Probability is represented by p}
    \\
    [2mm]
    \\
    \begin{math}
      H=\sum_{i=1}^np_i\times\#bounces_i\\
      [4mm]
      \#bounces=\log_2(\#outcomes)\\
      \#outcomes=\frac{1}{p}\\
      \#bounces = \log_2(\frac{1}{p})\\
      [4mm]
      H=\sum_{i=1}^np_i\times\log_2\frac{1}{p_i}\\
    \end{math}
    [4mm]
\end{document}
